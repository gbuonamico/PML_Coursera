testingHAR<-testingHAR[,-nsv]
testingHAR<-testingHAR[,7:d[2]]
validationHAR<-validationHAR[,-nsv]
validationHAR<-validationHAR[,7:d[2]]
set.seed(125)
doMC::registerDoMC(cores=4)
modFit <- train(classe ~ ., method="gbm",data=trainingHAR,verbose=FALSE,allowParallel=TRUE)
View(trainingHAR)
outlierTest(trainingHAR)
library(car)
install.packages("car")
library(car)
outlierTest(trainingHAR)
outlierTest(modFit)
lda <- train(classe ~ .,data=trainingHAR,method="lda")
outlierTest(lda)
qqPlot(lda, main="QQ Plot")
qqPlot(lda, main="QQ Plot")
qr(trainingHAR)$pivot
lm <- train(y ~ ., method="lm", data=trainingHAR)
lm <- train(classe ~ ., method="lm", data=trainingHAR)
plot(lda$finalModel)
plot(lda$finalModel)
nearZeroVar(trainingHAR,saveMetrics=F)
nearZeroVar(trainingHAR,saveMetrics=T)
trainknn<-preProcess(trainingHAR, method="knnImpute")
trainknn<-trainingHAR
trainknn[1]<-preProcess(trainingHAR[1], method="knnImpute")
trainknn[,1]<-preProcess(trainingHAR[,1], method="knnImpute")
View(trainknn)
trainknn[,5]<-preProcess(trainingHAR[,5], method="knnImpute")
preProcess(trainingHAR[,5], method="knnImpute")
selectNA <- rbinom(dim(trainingHAR)[1],size=1,prob=0.05)==1
preProcess(trainingHAR[,-58], method="knnImpute")
trainknn<-trainingHAR[,5:6]
preProcess(trainknn[,-58], method="knnImpute")
train2<-preProcess(trainknn[,-58], method="knnImpute")
train2
train2[1,]
train2[1]
testCapAveS <- (trainingHAR - mean(trainingHAR)) / sd(trainingHAR)
variable.sparseness <- apply(trainingHAR, 2, sparseness)
n
sparseness <- function(a) {
n <- length(a)
na.count <- sum(is.na(a))
return((n - na.count)/n)
}
variable.sparseness <- apply(trainingHAR, 2, sparseness)
trimTrainSub <- trainingHAR[, variable.sparseness > 0.9]
View(trimTrainSub)
modFit <- train(classe ~ ., method="gbm",data=trimTrainSub,verbose=FALSE,allowParallel=TRUE)
View(trimTrainSub)
modFit <- train(classe ~ ., method="rf",data=trimTrainSub,verbose=FALSE,allowParallel=TRUE)
print(modFit)
confusionMatrix(testingHAR$classe, predict(modFit, testingHAR))
resu<-confusionMatrix(testingHAR$classe, predict(modFit, testingHAR))
plot(resu)
resu
resu<-confusion(testingHAR$classe, predict(modFit, testingHAR))
resu.table
as.table(resu)
plot(as.table(resu))
ggplot(as.table(resu))
ggplot(as.data.frame(as.table(resu)))
normalize()
z<-as.table(resu)
colnames(z) = c("A","B","C","D","E")
rownames(z)=colnames(z)
image(z[,ncol(z):1], axes=FALSE)
heatmap(t(z)[ncol(z):1,], Rowv=NA,
Colv=NA, col = heat.colors(256))
plt.matshow(z)
heatmap(t(z)[ncol(z):1,], Rowv=NA,
Colv=NA, col = heat.colors(256))
heatmap(t(z)[ncol(z):1,], Rowv=NA,
Colv=NA, col = heat.colors(1256))
heatmap(t(z)[ncol(z):1,], Rowv=NA,
Colv=NA, col = heat.colors(64))
heatmap(t(z)[ncol(z):1,], Rowv=NA,
Colv=NA, col = heat.colors(64))
image(z[,ncol(z):1], axes=FALSE)
resuVAL<-confusion(validationHAR$classe, predict(modFit, validationHAR))
resuVAL<-confusionmatrix(validationHAR$classe, predict(modFit, validationHAR))
resuVAL<-confusionMatrix(validationHAR$classe, predict(modFit, validationHAR))
resuVAL
zval<-as.table(resuVAL)
zval
resuVAL<-confusionMatrix(inDatasetRefHAR$classe, predict(modFit, inDatasetRefHAR))
resuVAL<-confusionMatrix(inDatasetValidHAR$classe, predict(modFit, inDatasetValidHAR))
plot(density(trainingHAR$classe, col="blue")
)
nsv<-nearZeroVar(trainingHAR,saveMetrics=F)
nsv
nsv<-nearZeroVar(trainingHAR,saveMetrics=T)
nsv
library(caret)
library(ISLR); library(ggplot2);
# Create 1 datasets reading the CSV file
# We will Use this dataset both for training(70%) and testing(30%) purpose.
#
inDatasetHAR<-read.csv(file="pml-training.csv",head=TRUE,sep=",")
inDatasetValidHAR<-read.csv(file="pml-testing.csv",head=TRUE,sep=",")
#
#This is the data file that Velloso, E.; Bulling, A.; Gellersen, H.;
#Ugulino, W.; Fuks, H. use in thei paper "Qualitative Activity Recognition of
# Weight Lifting Exercises. Proceedings of 4th International Conference in
#Cooperation with SIGCHI (Augmented Human '13) .
##Stuttgart, Germany: ACM SIGCHI, 2013.
#
##Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3mvTxcXYG
#inDatasetRefHAR<-read.csv(file="dataset-har.csv",head=TRUE,sep=";")
#
#
#
inTrainTest <- createDataPartition(y=inDatasetHAR$classe,p=0.7, list=FALSE)
#70% of data for training and tests
TrTeHAR <- inDatasetHAR[inTrainTest,]
#Now Split in training and testing dataset (70% 30%)
trainingHAR<-inDatasetHAR[TrTeHAR,]
testingHAR<-inDatasetHAR[-TrTeHAR,]
trainingHAR<-inDatasetHAR[inTrainTest,]
testingHAR<-inDatasetHAR[-inTrainTest,]
##
## DATA ANALYSIS
##
# Analysis of training dataset : a lot of variable with "MISSING values", correlated and without variance
#
nsv<-nearZeroVar(trainingHAR,saveMetrics=T)
nsv
nsv(4)
nsv
nsv[4]
View(testingHAR)
View(trainingHAR)
nsv
trainingHAR<- trainingHAR[, -nsv]
nsv
nzv <- nearZeroVar(trainingHAR)
filteredDescr <- mdrrDescr[, -nzv]
dim(filteredDescr)
nzv <- nearZeroVar(trainingHAR)
filteredDescr <- trainingHAR[, -nzv]
dim(filteredDescr)
View(filteredDescr)
descrCor <-  cor(filteredDescr)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .999)
nsv2<-nearZeroVar(filteredDescr,saveMetrics=T)
nsv2
library(caret)
library(ISLR); library(ggplot2);
# Create 1 datasets reading the CSV file
# We will Use this dataset both for training(70%) and testing(30%) purpose.
#
inDatasetHAR<-read.csv(file="pml-training.csv",head=TRUE,sep=",")
inDatasetValidHAR<-read.csv(file="pml-testing.csv",head=TRUE,sep=",")
#
#This is the data file that Velloso, E.; Bulling, A.; Gellersen, H.;
#Ugulino, W.; Fuks, H. use in thei paper "Qualitative Activity Recognition of
# Weight Lifting Exercises. Proceedings of 4th International Conference in
#Cooperation with SIGCHI (Augmented Human '13) .
##Stuttgart, Germany: ACM SIGCHI, 2013.
#
##Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3mvTxcXYG
#inDatasetRefHAR<-read.csv(file="dataset-har.csv",head=TRUE,sep=";")
#
#
#
inTrainTest <- createDataPartition(y=inDatasetHAR$classe,p=0.7, list=FALSE)
#70% of data for training and tests
#Now Split in training and testing dataset (70% 30%)
trainingHAR<-inDatasetHAR[inTrainTest,]
testingHAR<-inDatasetHAR[-inTrainTest,]
##
## DATA ANALYSIS
##
#
# First 7 columns of the datasets can be skipped as they are not relevant for the analysis (name, window, ID...)
#
d<-dim(trainingHAR)
trainingHAR<-trainingHAR[,8:d[2]]
testingHAR<-testingHAR[,8:d[2]]
#
# Analysis of training dataset : a lot of variable with "MISSING values", correlated and without variance
#
#
#
#
#
nsv<-nearZeroVar(trainingHAR)
trainingHAR<- trainingHAR[, -nsv]
testingHAR<-testingHAR[,-nsv]
View(testingHAR)
nsv2<-nearZeroVar(trainingHAR,saveMetrics=T)
nsv2
is.na(traininhHAR)
is.na(trainingHAR)
listNA<-is.na(trainingHAR)
hist(listNA)
nsv2<-nearZeroVar(trainingHAR,saveMetrics=T)
nsv2
nsv2[2]
nsv2[2]>=5
sparseness <- function(a) {
n <- length(a)
na.count <- sum(is.na(a))
return((n - na.count)/n)
}
# sparness of input variables based on training subset
variable.sparseness <- apply(trainingHAR, 2, sparseness)
variable.sparseness
variable.sparseness[2]>0.9
variable.sparseness[2]
variable.sparseness>0.9
dim (variable)
View(testingHAR)
variable.sparseness>0.9
nsv2[2]>=2
nsv2[2]>=2==TRUE
print(nsv2,nsv2[2]>=2)
print(nsv2,freqRatio>=2)
print(nsv2,nsv2.freqRatio>=2)
print(nsv2[2]>=2)
print(nsv2[1])
nsv
print(nsv2[2]>=2)
variable.sparseness>0.9
c(nsv2[2])
c(nsv2[1])
c(nsv2)
c(nsv2==TRUE)
nsv2
checkConditionalX(trainingHAR, na)
checkConditionalX(trainingHAR, "NA")
l<-checkConditionalX(trainingHAR, "NA")
l
l<-trainingHAR[-c(nsv2[2]>2)]
l<-trainingHAR[-nsv2[2]>2]
l<-trainingHAR[-c(nsv2[2]>2)]
l<-trainingHAR[,-c(nsv2[2]>2)]
nsv2
nsv2$freqRatio>2
l<-trainingHAR[,-c(nsv2$freqRatio>2)]
l1<-trainingHAR[,-c(nsv2$freqRatio>2)]
View(l)
indexc<-c(nsv2$freqRatio>2)
indexc
c(indexc)
l1<-trainingHAR[,indexc]
l2<-trainingHAR[,-indexc]
-indexc
l2<-trainingHAR[,!indexc]
modFit <- train(classe ~ ., method="rf",data=l2,verbose=FALSE,allowParallel=TRUE)
print(modFit)
trimTrainSub <- trainingHAR[, variable.sparseness > 0.9]
modFit <- train(classe ~ ., method="rf",data=trimTrainSub,verbose=FALSE,allowParallel=TRUE)
print(modFit)
na_count <-sapply(trainingHAR, function(y) sum(length(which(is.na(y)))))
na_count
na_perc=na_count/dim(trainingHAR)
l2<- trainingHAR[, na_perc > 0.9]
modFit <- train(classe ~ ., method="rf",data=l2,verbose=FALSE,allowParallel=TRUE)
l2<- trainingHAR[, na_perc <= 0.9]
modFit <- train(classe ~ ., method="rf",data=l2,verbose=FALSE,allowParallel=TRUE)
print(modFit)
resu<-confusionMatrix(testingHAR$classe, predict(modFit, testingHAR))
resu
z<-as.table(resu)
colnames(z) = c("A","B","C","D","E")
rownames(z)=colnames(z)
image(z[,ncol(z):1], axes=FALSE)
heatmap(t(z)[ncol(z):1,], Rowv=NA,
+         Colv=NA, col = heat.colors(256))
image(z[,ncol(z):1], axes=FALSE)
heatmap(t(z)[ncol(z):1,], Rowv=NA,
+         Colv=NA, col = heat.colors(256))
heatmap(t(z)[ncol(z):1,], Rowv=NA,Colv=NA, col = heat.colors(256))
as.table(resu)
library(PerformanceAnalytics)
mydata <- mtcars[, c(1,3,4,5,6,7)]
chart.Correlation(mydata, histogram=TRUE, pch=19)
library(PerformanceAnalytics)
mydata <- trainingHAR[, c(8:20)]
chart.Correlation(mydata, histogram=TRUE, pch=19)
library(PerformanceAnalytics)
mydata <- trainingHAR[, c(8:20)]
chart.Correlation(mydata, histogram=TRUE, pch=19)
result <- rfcv(trainingHAR[,1:dim(trainingHAR)-1],trainingHAR[,dim(trainingHAR)],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)
dim(trainingHAR)
result <- rfcv(trainingHAR[,8:100],trainingHAR[,101],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)
na_count <-sapply(trainingHAR, function(y) sum(length(which(is.na(y)))))
na_perc=na_count/dim(trainingHAR)
reducedTrainingHAR<-trainingHAR[, na_perc <= 0.9]
reducedTestingHAR<-testingHAR[, na_perc <= 0.9]
result <- rfcv(reducedTrainingHAR[,1:52],reducedTrainingHAR[,53],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)
result
error.cv
result.error.cv
result$n.var
result$
result$error.cv
result$error.cv
result$predicted
prox=TRUE,allowParallel=TRUE)
modFit <- train(classe ~ ., method="rf",data=reducedTrainingHAR,trControl=trainControl(method="cv",number=5),prox=TRUE,allowParallel=TRUE)
result$error.cv
varImp(modFit)
result1 <- randomForest(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)
dim(reducedTrainingHAR)
dim(reducedTrainingHAR)[2]
result1 <- randomForest(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)
result1 <- randomForest(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), importance=TRUE)
result1 <- randomForest(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5, importance=TRUE)
varImp(result1)
varImp(result1$predicted)
varImp(result1$err.rate)
result1$err.rate
result1$predicted
result1$importance
result1$mtry
result1$ntree
result1$forest
result1$confusion
result1$importanceSD
plot(gbmImp, top = 20)
plot(result1, top = 20)
plot(result1)
plot(result1)
varImpPlot(result1,type=2)
varImpPlot(result1,type=1)
varImpPlot(result1)
varImpPlot(result1,type=1)
varImp(result1)
resu<-confusionMatrix(redreducedTestingHAR$classe, predict(result1, reducedTestingHAR))
resu<-confusionMatrix(reducedTestingHAR$classe, predict(result1, reducedTestingHAR))
resu
as.table(resu)
library(caret)
library(ISLR); library(ggplot2);
# Create 1 datasets reading the CSV file
# We will Use this dataset both for training(70%) and testing(30%) purpose.
#
inDatasetHAR<-read.csv(file="pml-training.csv",head=TRUE,sep=",")
inDatasetValidHAR<-read.csv(file="pml-testing.csv",head=TRUE,sep=",")
#
#This is the data file that Velloso, E.; Bulling, A.; Gellersen, H.;
#Ugulino, W.; Fuks, H. use in thei paper "Qualitative Activity Recognition of
# Weight Lifting Exercises. Proceedings of 4th International Conference in
#Cooperation with SIGCHI (Augmented Human '13) .
##Stuttgart, Germany: ACM SIGCHI, 2013.
#
##Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3mvTxcXYG
#inDatasetRefHAR<-read.csv(file="dataset-har.csv",head=TRUE,sep=";")
#
#
#
inTrainTest <- createDataPartition(y=inDatasetHAR$classe,p=0.7, list=FALSE)
#70% of data for training and tests
#Now Split in training and testing dataset (70% 30%)
trainingHAR<-inDatasetHAR[inTrainTest,]
testingHAR<-inDatasetHAR[-inTrainTest,]
##
## DATA ANALYSIS
##
#
# First 7 columns of the datasets can be skipped as they are not relevant for the analysis (name, window, ID...)
#
d<-dim(trainingHAR)
trainingHAR<-trainingHAR[,8:d[2]]
testingHAR<-testingHAR[,8:d[2]]
#
# Analysis of training dataset : a lot of variable with "MISSING values", correlated and without variance
# we can consider these as "poor" predictors
#
#
#
# Eliminates zero Variance predictors..
nsv<-nearZeroVar(trainingHAR)
trainingHAR<- trainingHAR[, -nsv]
testingHAR<-testingHAR[,-nsv]
##
## Then eliminates predictors with imprtant percentage of missing values (>90%)
##
##
na_count <-sapply(trainingHAR, function(y) sum(length(which(is.na(y)))))
na_perc=na_count/dim(trainingHAR)
reducedTrainingHAR<-trainingHAR[, na_perc <= 0.9]
reducedTestingHAR<-testingHAR[, na_perc <= 0.9]
set.seed(125)
doMC::registerDoMC(cores=4)
FitModel <- rfcv(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)
library(MASS)
FitModel <- rfcv(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)
library(randomForest)
library(class)
FitModel <- rfcv(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)
FitModel$error.cv
print(FitModel)
FitModel
FitModel$n.var
varimp(FitModel)
varImp(FitModel)
FitModel$importance
FitModel$predicted
FitModel$n.var
FitModel <- randomForest(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5, importance=TRUE)
FitModel
resu<-confusionMatrix(redreducedTestingHAR$classe, predict(FitMidel, reducedTestingHAR))
resu<-confusionMatrix(reducedTestingHAR$classe, predict(FitMidel, reducedTestingHAR))
resu<-confusionMatrix(reducedTestingHAR$classe, predict(FitModel, reducedTestingHAR))
resu
z<-as.table(resu)
colnames(z) = c("A","B","C","D","E")
rownames(z)=colnames(z)
image(z[,ncol(z):1], axes=FALSE)
heatmap(t(z)[ncol(z):1,], Rowv=NA,Colv=NA, col = heat.colors(256))
plot(FitModel)
plot(FitModel$importance)
FitModel$importance
predict20=predict(FitModel,inDatasetValidHAR)
View(inDatasetValidHAR)
20outdata<-inDatasetValidHAR[,8:d[2]]
outdata20<-inDatasetValidHAR[,8:d[2]]
outdata20<-outdata2[,-nsv]
outdata20<-outdata20[,-nsv]
outdata20<-outdata20[, na_perc <= 0.9]
predict20=predict(FitModel,outdata20)
predict20
plot(FitModel$importance;type=2)
plot(FitModel$importance,type=2)
varImpPlot(ModelFit,type=2)
varImpPlot(FitModel,type=2)
FitModel$importance
plot(FitModel$importance)
plot(FitModel)
FitModel$error.cv
FitModel$err.rate
FitModel$importance
VarImp(FitModel)
Varimp(FitModel)
varImp(FitModel)
rf variable importance()
imp <- varImp(ModelFit)
rownames(imp)[order(imp$Overall, decreasing=TRUE)]
imp <- varImp(ModelFit)
imp <- varImp(FitModel)
rownames(imp)[order(imp$Overall, decreasing=TRUE)]
imp <- varImp(FitModel$importance)
imp <- FitModel$importance
rownames(imp)[order(imp$Overall, decreasing=TRUE)]
FitModel2 <- train(classe ~ ., method="rf",data=reducedTrainingHAR,trControl=trainControl(method="cv",number=5),prox=TRUE,importance=TRUE,allowParallel=TRUE)
plot(FitModel2)
plot(FitModel)
plot(FitModel2)
plot(FitModel$importance)
varImpPlot(FitModel,type=2)
varImpPlot(FitModel)
varImpPlot(FitModel2)
FitModel2$results
FitModel2
FitModel
plot(FitModel2)
plot(FitModel$importance)
plot(FitModel2)
FitModel2$results
varImpPlot(FitModel)
FitModel2$results
resu<-confusionMatrix(reducedTestingHAR$classe, predict(FitModel2, reducedTestingHAR))
resu
z<-as.table(resu)
z
resu
z<-as.table(resu)
colnames(z) = c("A","B","C","D","E")
rownames(z)=colnames(z)
image(z[,ncol(z):1], axes=FALSE)
heatmap(t(z)[ncol(z):1,], Rowv=NA,Colv=NA, col = heat.colors(256))
outdata20<-inDatasetValidHAR[,8:d[2]]
outdata20<-outdata20[,-nsv]
outdata20<-outdata20[, na_perc <= 0.9]
predict20=predict(FitModel,outdata20)
predict20
FitModel0 <- rfcv(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE,importance=TRUE)
dim(predict20)
predict20
predict20[1]
predict20[2]
predict20.c
c(predict20)
c(predict20[2])
predict20
answer[1]=B
answer[1]="B"
dim(answer[1:20])
factor(predict20)
as.character(predict20)
answer<-as.character(predict20)
dim answer
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answer)
output: html_document
output: html_document
title: "Assignment Practical Machine Learning Course"
plot(FitModel2)
plot(FitModel2)
plot(FitModel2)

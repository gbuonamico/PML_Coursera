---
title: "Assignment Practical Machine Learning Course"
output: html_document
---

Assignment for the Practical Machine Learning Course
Published on 27/09/2015 By Lupo Argentato (Facebook Pseudo) alias Gianluca BUONAMICO


STEP 1: What is the question?

Data from a sample of 6 people using accelerometers on the belt, forearm, arm, and dumbell are used to understand how well they are performing compared to a 'witness' group of professionnals store in A group

From collected observations, the aim is to buld a predictive model that will assess how well the exercise is done and rank in the right group, from A to E
#


STEP 2 : LOAD AND ANALYSE DATA
```{r}
## 

# load relevant libraries
library(caret)
library(ISLR); library(ggplot2);
# Create 1 datasets reading the CSV file
# We will Use this dataset both for training(70%) and testing(30%) purpose.
# 
inDatasetHAR<-read.csv(file="pml-training.csv",head=TRUE,sep=",")
inDatasetValidHAR<-read.csv(file="pml-testing.csv",head=TRUE,sep=",")
#
```
This is the data file that Velloso, E.; Bulling, A.; Gellersen, H.; 
Ugulino, W.; Fuks, H. use in thei paper "Qualitative Activity Recognition of 
Weight Lifting Exercises. Proceedings of 4th International Conference in 
Cooperation with SIGCHI (Augmented Human '13) . 
Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3mvTxcXYG

Now Load data and store in two different datasets, for training and testing purpose
```{r}
inTrainTest <- createDataPartition(y=inDatasetHAR$classe,p=0.7, list=FALSE)
#70% of data for training and tests
#Now Split in training and testing dataset (70% 30%)
trainingHAR<-inDatasetHAR[inTrainTest,]
testingHAR<-inDatasetHAR[-inTrainTest,]

```
So we choose to have a training SET for 70% of data and 30% for testing. The dataset contains 19622 observations and 160 variables.
First 7 variables are probably not relevant (name, ID line, Window...). So we throw them out


```{r}
d<-dim(trainingHAR)
trainingHAR<-trainingHAR[,8:d[2]]
testingHAR<-testingHAR[,8:d[2]]
```

Analysis of training dataset : a lot of variable with "MISSING values", and variables with zero variance
we can consider these as "poor" predictors
```{r}
# Eliminates zero Variance predictors..
nearZeroVar(trainingHAR,saveMetrics=TRUE)
nsv<-nearZeroVar(trainingHAR)
trainingHAR<- trainingHAR[, -nsv]
testingHAR<-testingHAR[,-nsv]

```
Variable with missing values are also frequent. We measur the quantity of these NA values and put a threshold to 90%. Then we will keep only variables with less than 90% of NA values
```{r}
##
## Then eliminates predictors with imprtant percentage of missing values (>90%)
##
##
na_count <-sapply(trainingHAR, function(y) sum(length(which(is.na(y)))))
na_perc=na_count/dim(trainingHAR)
reducedTrainingHAR<-trainingHAR[, na_perc <= 0.9]
reducedTestingHAR<-testingHAR[, na_perc <= 0.9]
```

So 53 selected Predictors of 160 remains for building the prediction model  

STEP 3 The Algorythm

Random Forest used with cross validation seems to be a good approach, because the class of alghorithm fit well this kind of classification problem, bootstrap and cross validation are also easy to use.
We can estimate how the number of predictors will influence the error of the model using the rfcv,randomForest or train function in caret. the differences are in execution time and output variables, and the use with the varImp output to estimate importances of predictors and errors outputs.
The train function is the most time consuming.
```{r}
# set seed value and parallel options
set.seed(125)
doMC::registerDoMC(cores=4)
library(MASS)
library(randomForest)
library(class)

#FitModel <- rfcv(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)

#
# Alternatively, we can use this
#FitModel0 <- randomForest(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5, importance=TRUE)
#FitModel2 <- train(classe ~ ., method="rf",data=reducedTrainingHAR,trControl=trainControl(method="cv",number=5),prox=TRUE,importance=TRUE,allowParallel=TRUE)

```
Plotting FitModel2 shows that the error decrease is not very important after 26 predictors chosen by the alghorythm
FitModel (obtained with rfcv function)shows also how errors decrease with nÂ° trees for each class

```{r, echo=FALSE}

#plot(FitModel2)
#plot(FitModel)

```

also print the most important predictors

```{r, echo=FALSE}
#varImpPlot(FitModel)

```


The model result shows a high accuracy in the training set.
```{r}
#FitModel2

```

STEP4 EVALUATION

Lets try it on the testing set and use confusion matrix to assess the score

```{r}
#resu<-confusionMatrix(reducedTestingHAR$classe, predict(FitModel2, reducedTestingHAR))
#resu
#z<-as.table(resu)
#z


# try also a heatmap but not really nice
# colnames(z) = c("A","B","C","D","E")
# rownames(z)=colnames(z)
# image(z[,ncol(z):1], axes=FALSE)
# heatmap(t(z)[ncol(z):1,], Rowv=NA,Colv=NA, col = heat.colors(256))
#
```
Balanced accuracy is really nice.

Let's finish with the 20 observations loaded in the file (let's apply same reduction to variables)
```{r}
outdata20<-inDatasetValidHAR[,8:d[2]]
outdata20<-outdata20[,-nsv]
outdata20<-outdata20[, na_perc <= 0.9]
#predict20=predict(FitModel,outdata20)
#predict20


```

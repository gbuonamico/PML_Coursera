{
    "contents" : "---\noutput: html_document\n---\ntitle: \"Assignment Practical Machine Learning Course\"\noutput: html_document\n---\n\nAssignment for the Practical Machine Learning Course\nPublished on 27/09/2015 By Lupo Argentato (Facebook Pseudo) alias Gianluca BUONAMICO\n\n\nSTEP 1: What is the question?\n\nData from a sample of 6 people using accelerometers on the belt, forearm, arm, and dumbell are used to understand how well they are performing compared to a 'witness' group of professionnals store in A group\n\nFrom collected observations, the aim is to buld a predictive model that will assess how well the exercise is done and rank in the right group, from A to E\n#\n\n\nSTEP 2 : LOAD AND ANALYSE DATA\n```{r}\n## \n\n# load relevant libraries\nlibrary(caret)\nlibrary(ISLR); library(ggplot2);\n# Create 1 datasets reading the CSV file\n# We will Use this dataset both for training(70%) and testing(30%) purpose.\n# \ninDatasetHAR<-read.csv(file=\"pml-training.csv\",head=TRUE,sep=\",\")\ninDatasetValidHAR<-read.csv(file=\"pml-testing.csv\",head=TRUE,sep=\",\")\n#\n```\nThis is the data file that Velloso, E.; Bulling, A.; Gellersen, H.; \nUgulino, W.; Fuks, H. use in thei paper \"Qualitative Activity Recognition of \nWeight Lifting Exercises. Proceedings of 4th International Conference in \nCooperation with SIGCHI (Augmented Human '13) . \nStuttgart, Germany: ACM SIGCHI, 2013.\n\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz3mvTxcXYG\n\nNow Load data and store in two different datasets, for training and testing purpose\n```{r}\ninTrainTest <- createDataPartition(y=inDatasetHAR$classe,p=0.7, list=FALSE)\n#70% of data for training and tests\n#Now Split in training and testing dataset (70% 30%)\ntrainingHAR<-inDatasetHAR[inTrainTest,]\ntestingHAR<-inDatasetHAR[-inTrainTest,]\n\n```\nSo we choose to have a training SET for 70% of data and 30% for testing. The dataset contains 19622 observations and 160 variables.\nFirst 7 variables are probably not relevant (name, ID line, Window...). So we throw them out\n\n\n```{r}\nd<-dim(trainingHAR)\ntrainingHAR<-trainingHAR[,8:d[2]]\ntestingHAR<-testingHAR[,8:d[2]]\n```\n\nAnalysis of training dataset : a lot of variable with \"MISSING values\", and variables with zero variance\nwe can consider these as \"poor\" predictors\n```{r}\n# Eliminates zero Variance predictors..\nnearZeroVar(trainingHAR,saveMetrics=TRUE)\nnsv<-nearZeroVar(trainingHAR)\ntrainingHAR<- trainingHAR[, -nsv]\ntestingHAR<-testingHAR[,-nsv]\n\n```\nVariable with missing values are also frequent. We measur the quantity of these NA values and put a threshold to 90%. Then we will keep only variables with less than 90% of NA values\n```{r}\n##\n## Then eliminates predictors with imprtant percentage of missing values (>90%)\n##\n##\nna_count <-sapply(trainingHAR, function(y) sum(length(which(is.na(y)))))\nna_perc=na_count/dim(trainingHAR)\nreducedTrainingHAR<-trainingHAR[, na_perc <= 0.9]\nreducedTestingHAR<-testingHAR[, na_perc <= 0.9]\n```\n\nSo 53 selected Predictors of 160 remains for building the prediction model  \n\nSTEP 3 The Algorythm\n\nRandom Forest used with cross validation seems to be a good approach, because the class of alghorithm fit well this kind of classification problem, bootstrap and cross validation are also easy to use.\nWe can estimate how the number of predictors will influence the error of the model using the rfcv,randomForest or train function in caret. the differences are in execution time and output variables, and the use with the varImp output to estimate importances of predictors and errors outputs.\nThe train function is the most time consuming.\n```{r}\n# set seed value and parallel options\nset.seed(125)\ndoMC::registerDoMC(cores=4)\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(class)\n\n#FitModel <- rfcv(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5,mtry=function(p) max(1, floor(sqrt(p))), recursive=TRUE)\n\n#\n# Alternatively, we can use this\n#FitModel <- randomForest(reducedTrainingHAR[,1:(dim(reducedTrainingHAR)[2]-1)],reducedTrainingHAR[,dim(reducedTrainingHAR)[2]],cv.fold=5, importance=TRUE)\n\n#FitModel2 <- train(classe ~ ., method=\"rf\",data=reducedTrainingHAR,trControl=trainControl(method=\"cv\",number=5),prox=TRUE,importance=TRUE,allowParallel=TRUE)\n\n```\nPlotting FitModel2 shows that the error decrease is not very important after 26 predictors chosen by the alghorythm\nFitModel (obtained with rfcv function)shows also how errors decrease with nÂ° trees for each class\n\n```{r echo=off}\nplot(FitModel2)\nplot(FitModel)\n```\n\nalso print the most important predictors\n\n```{r echo=off}\nvarImpPlot(FitModel)\n\n```\n\n\nThe model result shows a high accuracy in the training set.\n```{r}\nFitModel2\n\n```\nSTEP4 EVALUATION\nLet's try it on the testing set and use confusion matrix to assess the score\n```{r}\nresu<-confusionMatrix(reducedTestingHAR$classe, predict(FitModel2, reducedTestingHAR))\nresu\nz<-as.table(resu)\nz\n\n\n# try also a heatmap but not really nice\n# colnames(z) = c(\"A\",\"B\",\"C\",\"D\",\"E\")\n# rownames(z)=colnames(z)\n# image(z[,ncol(z):1], axes=FALSE)\n# heatmap(t(z)[ncol(z):1,], Rowv=NA,Colv=NA, col = heat.colors(256))\n#\n```\nBalanced accuracy is really nice.\n\nLet's finish with the 20 observations loaded in the file (let's apply same reduction to variables)\n```{r}\noutdata20<-inDatasetValidHAR[,8:d[2]]\noutdata20<-outdata20[,-nsv]\noutdata20<-outdata20[, na_perc <= 0.9]\npredict20=predict(FitModel,outdata20)\n#predict20\n\n\n```",
    "created" : 1443391303121.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "254477297",
    "id" : "74347986",
    "lastKnownWriteTime" : 1443391539,
    "path" : "~/Documents/PML/AssignmentPML_GB.Rmd",
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}